{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNCSG8il3cu8",
        "outputId": "3841ff41-6b59-4113-f5bb-848fa6fd21ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 3])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.4300, 0.1500, 0.8910],\n",
              "         [0.5500, 0.8700, 0.6600],\n",
              "         [0.5700, 0.8500, 0.6410],\n",
              "         [0.2200, 0.5800, 0.3310],\n",
              "         [0.7700, 0.2500, 0.1010],\n",
              "         [0.0500, 0.8000, 0.5510]],\n",
              "\n",
              "        [[0.4300, 0.1500, 0.8910],\n",
              "         [0.5500, 0.8700, 0.6600],\n",
              "         [0.5700, 0.8500, 0.6410],\n",
              "         [0.2200, 0.5800, 0.3310],\n",
              "         [0.7700, 0.2500, 0.1010],\n",
              "         [0.0500, 0.8000, 0.5510]]])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "inputs = torch.tensor([\n",
        "    [0.43, 0.15, 0.891],  # \"Your\"\n",
        "    [0.55, 0.87, 0.66],   # \"journey\"\n",
        "    [0.57, 0.85, 0.641],  # \"starts\"\n",
        "    [0.22, 0.58, 0.331], # \"with\"\n",
        "    [0.77, 0.25, 0.101],  # \"one\"\n",
        "    [0.05, 0.80, 0.551]   # \"step\"\n",
        "])\n",
        "\n",
        "print(inputs.shape)\n",
        "\n",
        "input_batch = torch.stack((inputs,inputs),dim=0)\n",
        "input_batch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedSelfAttention(nn.Module):\n",
        "  def __init__(self,in_dim,out_dim,context_len,bias=False,dropout=0.25):\n",
        "    super().__init__()\n",
        "    self.d_out = out_dim\n",
        "    self.W_K = nn.Linear(in_dim,out_dim,bias= False)\n",
        "    self.W_Q = nn.Linear(in_dim,out_dim,bias= False)\n",
        "    self.W_V = nn.Linear(in_dim,out_dim,bias= False)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.register_buffer('mask',torch.triu(torch.ones(context_len,context_len),diagonal=1))\n",
        "  def forward(self,x):\n",
        "    b, num_tokens , d_in = x.shape\n",
        "    keys = self.W_K(x)\n",
        "    queries = self.W_Q(x)\n",
        "    values = self.W_V(x)\n",
        "    attention_scores = queries @ keys.transpose(1,2)# here if shape is (2,6,2) , it will swap 1st index dimension and 2nd index dimension.\n",
        "    attention_scores.masked_fill_(\n",
        "        self.mask.bool()[:num_tokens,:num_tokens],-torch.inf\n",
        "    )\n",
        "    attention_weights  = torch.softmax(attention_scores/keys.shape[-1]**0.5,dim=-1)\n",
        "    attention_weights =self.dropout(attention_weights)\n",
        "    return attention_weights @ values"
      ],
      "metadata": {
        "id": "czJTNifPWfyS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "  def __init__(self,d_in,d_out,context_len,dropout,num_heads,qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([\n",
        "        MaskedSelfAttention(d_in,d_out,context_len,dropout=dropout,bias=qkv_bias) for _ in range(num_heads)\n",
        "    ])\n",
        "  def forward(self,input_batch):\n",
        "    return torch.cat([head(input_batch) for head in self.heads],dim=-1\n",
        "                     )\n"
      ],
      "metadata": {
        "id": "g5nXqlzNqDEs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "d_in , d_out = 3,2 # vector embedding dimension , key matrix dimensioncla\n",
        "multi_attention = MultiHeadAttentionWrapper(d_in,d_out,6,0,2)\n"
      ],
      "metadata": {
        "id": "ZImG4IU-simb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multi_attention(input_batch).shape"
      ],
      "metadata": {
        "id": "VQ35SZ9sp-0K",
        "outputId": "06af355a-df4c-45e0-a948-6458ff28e243",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 6, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multi_attention(input_batch)"
      ],
      "metadata": {
        "id": "DMGowxGJrazD",
        "outputId": "970d565a-25b1-4813-cbb6-636758d0fe23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.4521,  0.2220,  0.4774,  0.1063],\n",
              "         [-0.5791,  0.0195,  0.5771,  0.3018],\n",
              "         [-0.6227, -0.0510,  0.6102,  0.3660],\n",
              "         [-0.5671, -0.0790,  0.5471,  0.3513],\n",
              "         [-0.5503, -0.0916,  0.5337,  0.3406],\n",
              "         [-0.5309, -0.1039,  0.5074,  0.3425]],\n",
              "\n",
              "        [[-0.4521,  0.2220,  0.4774,  0.1063],\n",
              "         [-0.5791,  0.0195,  0.5771,  0.3018],\n",
              "         [-0.6227, -0.0510,  0.6102,  0.3660],\n",
              "         [-0.5671, -0.0790,  0.5471,  0.3513],\n",
              "         [-0.5503, -0.0916,  0.5337,  0.3406],\n",
              "         [-0.5309, -0.1039,  0.5074,  0.3425]]], grad_fn=<CatBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Multihead Attention With weight splits"
      ],
      "metadata": {
        "id": "zt4lQqMav8p2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "# Define the tensor with 3 rows and 6 columns\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89, 0.55, 0.87, 0.66],  # Row 1\n",
        "     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],  # Row 2\n",
        "     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]  # Row 3\n",
        ")\n",
        "inputs.shape"
      ],
      "metadata": {
        "id": "iOV4McRbtPj3",
        "outputId": "1ba75f1a-587b-408a-bbad-766e588c4a2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_batch = torch.stack((inputs,inputs),dim=0)\n",
        "b,n_tokens,_ = input_batch.shape"
      ],
      "metadata": {
        "id": "YLWV96CqWbkP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_out , num_heads =6,2\n",
        "#initialize weight matrices mxn m should be same as token embedding dimension and n is d_out\n",
        "query_weights = nn.Linear(input_batch.shape[-1],d_out)\n",
        "key_weights = nn.Linear(input_batch.shape[-1],d_out)\n",
        "value_weights = nn.Linear(input_batch.shape[-1],d_out)"
      ],
      "metadata": {
        "id": "ZHyzw_8LWt22"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keys = key_weights(input_batch)\n",
        "values = value_weights(input_batch)\n",
        "queries = query_weights(input_batch)"
      ],
      "metadata": {
        "id": "z7Trg35ZW_Qv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keys"
      ],
      "metadata": {
        "id": "_uAgGRBWX5vu",
        "outputId": "64a69962-3436-43e1-d4a8-9cab474447d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.0169,  0.8474, -0.1203, -0.5320,  0.2600, -0.1741],\n",
              "         [ 0.2498,  0.9276, -0.3431, -0.0579,  0.0654, -0.4072],\n",
              "         [ 0.0256,  0.8131, -0.1163, -0.0246,  0.3894, -0.4116]],\n",
              "\n",
              "        [[-0.0169,  0.8474, -0.1203, -0.5320,  0.2600, -0.1741],\n",
              "         [ 0.2498,  0.9276, -0.3431, -0.0579,  0.0654, -0.4072],\n",
              "         [ 0.0256,  0.8131, -0.1163, -0.0246,  0.3894, -0.4116]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "head_dim = int((d_out/num_heads))\n",
        "n_heads = 2\n",
        "#introduces\n",
        "keys = keys.view(b,n_tokens,n_heads,head_dim)\n",
        "values = values.view(b,n_tokens,n_heads,head_dim)\n",
        "queries = queries.view(b,n_tokens,n_heads,head_dim)\n",
        "print(keys,\"\\n\",keys.shape)"
      ],
      "metadata": {
        "id": "8AcNpuX-Yba2",
        "outputId": "a42cd723-4729-4122-a308-9cc44f4bf761",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[-0.0169,  0.8474, -0.1203],\n",
            "          [-0.5320,  0.2600, -0.1741]],\n",
            "\n",
            "         [[ 0.2498,  0.9276, -0.3431],\n",
            "          [-0.0579,  0.0654, -0.4072]],\n",
            "\n",
            "         [[ 0.0256,  0.8131, -0.1163],\n",
            "          [-0.0246,  0.3894, -0.4116]]],\n",
            "\n",
            "\n",
            "        [[[-0.0169,  0.8474, -0.1203],\n",
            "          [-0.5320,  0.2600, -0.1741]],\n",
            "\n",
            "         [[ 0.2498,  0.9276, -0.3431],\n",
            "          [-0.0579,  0.0654, -0.4072]],\n",
            "\n",
            "         [[ 0.0256,  0.8131, -0.1163],\n",
            "          [-0.0246,  0.3894, -0.4116]]]], grad_fn=<ViewBackward0>) \n",
            " torch.Size([2, 3, 2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#group matrices by number of heads which is at index 2  torch.Size([2, 3, 2, 3])\n",
        "keys = keys.transpose(1,2)\n",
        "values = values.transpose(1,2)\n",
        "queries = queries.transpose(1,2)"
      ],
      "metadata": {
        "id": "zNu1XDDHZXjJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(keys,\"\\n\",keys.shape)"
      ],
      "metadata": {
        "id": "jj7zorcEa2d5",
        "outputId": "14926ae8-ada0-48f6-a312-85477c531591",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[-0.0169,  0.8474, -0.1203],\n",
            "          [ 0.2498,  0.9276, -0.3431],\n",
            "          [ 0.0256,  0.8131, -0.1163]],\n",
            "\n",
            "         [[-0.5320,  0.2600, -0.1741],\n",
            "          [-0.0579,  0.0654, -0.4072],\n",
            "          [-0.0246,  0.3894, -0.4116]]],\n",
            "\n",
            "\n",
            "        [[[-0.0169,  0.8474, -0.1203],\n",
            "          [ 0.2498,  0.9276, -0.3431],\n",
            "          [ 0.0256,  0.8131, -0.1163]],\n",
            "\n",
            "         [[-0.5320,  0.2600, -0.1741],\n",
            "          [-0.0579,  0.0654, -0.4072],\n",
            "          [-0.0246,  0.3894, -0.4116]]]], grad_fn=<TransposeBackward0>) \n",
            " torch.Size([2, 2, 3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compute attention scores\n",
        "attention_scores = queries @ keys.transpose(-1,-2)\n",
        "attention_scores"
      ],
      "metadata": {
        "id": "f3njIEmCb7NZ",
        "outputId": "a0ff599b-b39f-4d40-e37d-900cfe6823b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-0.2827, -0.3169, -0.2963],\n",
              "          [-0.4942, -0.5187, -0.4996],\n",
              "          [-0.3080, -0.3412, -0.3198]],\n",
              "\n",
              "         [[-0.4602, -0.2366, -0.2496],\n",
              "          [-0.3532, -0.2252, -0.2302],\n",
              "          [-0.2715, -0.1768, -0.2114]]],\n",
              "\n",
              "\n",
              "        [[[-0.2827, -0.3169, -0.2963],\n",
              "          [-0.4942, -0.5187, -0.4996],\n",
              "          [-0.3080, -0.3412, -0.3198]],\n",
              "\n",
              "         [[-0.4602, -0.2366, -0.2496],\n",
              "          [-0.3532, -0.2252, -0.2302],\n",
              "          [-0.2715, -0.1768, -0.2114]]]], grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#finding attention weights , normalizing\n",
        "scores = attention_scores / (head_dim ** 0.5)\n",
        "\n",
        "#mask is n_tokensxn_tokens because attention scores arae between token and token\n",
        "mask = torch.triu(torch.ones(n_tokens, n_tokens), diagonal=1).bool()  # (T, T)\n",
        "scores = scores.masked_fill(mask.unsqueeze(0), float(\"-inf\"))\n",
        "\n",
        "attention_weights = torch.softmax(scores, dim=-1)"
      ],
      "metadata": {
        "id": "YGEixPiAcn-l"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(attention_weights,\"\\n\",attention_weights.shape)"
      ],
      "metadata": {
        "id": "MwhIj3kKco9s",
        "outputId": "5950d54a-89e5-402d-a57a-c85b6b8db630",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[1.0000, 0.0000, 0.0000],\n",
            "          [0.5035, 0.4965, 0.0000],\n",
            "          [0.3362, 0.3298, 0.3339]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4815, 0.5185, 0.0000],\n",
            "          [0.3235, 0.3417, 0.3349]]],\n",
            "\n",
            "\n",
            "        [[[1.0000, 0.0000, 0.0000],\n",
            "          [0.5035, 0.4965, 0.0000],\n",
            "          [0.3362, 0.3298, 0.3339]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4815, 0.5185, 0.0000],\n",
            "          [0.3235, 0.3417, 0.3349]]]], grad_fn=<SoftmaxBackward0>) \n",
            " torch.Size([2, 2, 3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_vectors = attention_weights @ values\n",
        "print(context_vectors,\"\\n\",context_vectors.shape)"
      ],
      "metadata": {
        "id": "Tmt-vG5YerJv",
        "outputId": "c1cd4682-a713-4c40-d61c-1d5c47ee25be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[ 0.0081,  0.2463,  0.9245],\n",
            "          [-0.1415,  0.2808,  1.0286],\n",
            "          [-0.1234,  0.1638,  0.9805]],\n",
            "\n",
            "         [[-0.0649,  0.7756, -0.1287],\n",
            "          [-0.1837,  0.7628, -0.1644],\n",
            "          [-0.1185,  0.7449, -0.0999]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0081,  0.2463,  0.9245],\n",
            "          [-0.1415,  0.2808,  1.0286],\n",
            "          [-0.1234,  0.1638,  0.9805]],\n",
            "\n",
            "         [[-0.0649,  0.7756, -0.1287],\n",
            "          [-0.1837,  0.7628, -0.1644],\n",
            "          [-0.1185,  0.7449, -0.0999]]]], grad_fn=<UnsafeViewBackward0>) \n",
            " torch.Size([2, 2, 3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#in order to concat two attention heads we have to bring them to gether in shape\n",
        "# torch.Size([2, 2, 3, 3]) [b,attn_heads,vectorsm]\n",
        "context_vectors.contiguous().view(b, n_tokens, 6)"
      ],
      "metadata": {
        "id": "BFuZXQ67fTal",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1851dae8-d25a-4359-f4df-a282da376ae4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0081,  0.2463,  0.9245, -0.1415,  0.2808,  1.0286],\n",
              "         [-0.1234,  0.1638,  0.9805, -0.0649,  0.7756, -0.1287],\n",
              "         [-0.1837,  0.7628, -0.1644, -0.1185,  0.7449, -0.0999]],\n",
              "\n",
              "        [[ 0.0081,  0.2463,  0.9245, -0.1415,  0.2808,  1.0286],\n",
              "         [-0.1234,  0.1638,  0.9805, -0.0649,  0.7756, -0.1287],\n",
              "         [-0.1837,  0.7628, -0.1644, -0.1185,  0.7449, -0.0999]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_vectors.reshape(2,3,6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ngn1gdPzHQFU",
        "outputId": "8fe6d330-084a-4e37-e897-8a9cd956dd21"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0081,  0.2463,  0.9245, -0.1415,  0.2808,  1.0286],\n",
              "         [-0.1234,  0.1638,  0.9805, -0.0649,  0.7756, -0.1287],\n",
              "         [-0.1837,  0.7628, -0.1644, -0.1185,  0.7449, -0.0999]],\n",
              "\n",
              "        [[ 0.0081,  0.2463,  0.9245, -0.1415,  0.2808,  1.0286],\n",
              "         [-0.1234,  0.1638,  0.9805, -0.0649,  0.7756, -0.1287],\n",
              "         [-0.1837,  0.7628, -0.1644, -0.1185,  0.7449, -0.0999]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_vectors.contiguous().view(b, n_tokens, d_out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTE3fps7Hamm",
        "outputId": "d35a26d9-845a-42f8-b556-a44986379dd2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0081,  0.2463,  0.9245, -0.1415,  0.2808,  1.0286],\n",
              "         [-0.1234,  0.1638,  0.9805, -0.0649,  0.7756, -0.1287],\n",
              "         [-0.1837,  0.7628, -0.1644, -0.1185,  0.7449, -0.0999]],\n",
              "\n",
              "        [[ 0.0081,  0.2463,  0.9245, -0.1415,  0.2808,  1.0286],\n",
              "         [-0.1234,  0.1638,  0.9805, -0.0649,  0.7756, -0.1287],\n",
              "         [-0.1837,  0.7628, -0.1644, -0.1185,  0.7449, -0.0999]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_vectors.contiguous().view(b,n_tokens,d_out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y76r2i1SKqkY",
        "outputId": "eff25548-abbf-4b50-8ac5-83229b68d8b7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0081,  0.2463,  0.9245, -0.1415,  0.2808,  1.0286],\n",
              "         [-0.1234,  0.1638,  0.9805, -0.0649,  0.7756, -0.1287],\n",
              "         [-0.1837,  0.7628, -0.1644, -0.1185,  0.7449, -0.0999]],\n",
              "\n",
              "        [[ 0.0081,  0.2463,  0.9245, -0.1415,  0.2808,  1.0286],\n",
              "         [-0.1234,  0.1638,  0.9805, -0.0649,  0.7756, -0.1287],\n",
              "         [-0.1837,  0.7628, -0.1644, -0.1185,  0.7449, -0.0999]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "      super().__init__()\n",
        "      assert (d_out % num_heads == 0), \\\n",
        "          \"d_out must be divisible by num_heads\"\n",
        "      self.d_out = d_out\n",
        "      self.num_heads = num_heads\n",
        "      self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "      self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "      self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "      self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "      self.out_proj = nn.Linear(d_out,d_out)\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "      self.register_buffer(\n",
        "          \"mask\",\n",
        "          torch.triu(torch.ones(context_length,context_length),diagonal=1)\n",
        "      )\n",
        "    def forward(self,x):\n",
        "      b , num_tokens, d_in = x.shape\n",
        "      print(x.shape)\n",
        "      queries = self.W_query(x).view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "      keys = self.W_key(x).view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "      values = self.W_value(x).view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "      # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "      keys = keys.transpose(1,2)\n",
        "      values = values.transpose(1,2)\n",
        "      queries = queries.transpose(1,2)\n",
        "\n",
        "\n",
        "      attention_scores = queries @ keys.transpose(-1,-2)\n",
        "      # Original mask truncated to the number of tokens and converted to boolean\n",
        "      mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "      attention_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "      attn_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "      attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "      # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "      context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "      # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "      context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "      context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "      return context_vec\n"
      ],
      "metadata": {
        "id": "b4uEr7xAKr2Z"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_in,d_out =6,6\n",
        "attention = MultiHeadAttention(d_in,d_out,6,0.25,2)"
      ],
      "metadata": {
        "id": "gWXX25LoOxaI"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention.forward(input_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzNONK-OOz_h",
        "outputId": "ed09be2f-b23a-44ed-c036-791b967ec3d6"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 6])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.2690, -0.1768,  0.5210,  0.2621,  0.0503, -0.5156],\n",
              "         [-0.0447, -0.3097,  0.3205,  0.1767, -0.1432, -0.3263],\n",
              "         [ 0.0896, -0.1831,  0.4364,  0.2409,  0.0094, -0.4319]],\n",
              "\n",
              "        [[ 0.2690, -0.1768,  0.5210,  0.2621,  0.0503, -0.5156],\n",
              "         [ 0.1739, -0.1214,  0.5048,  0.1656,  0.0788, -0.4511],\n",
              "         [ 0.0756, -0.2017,  0.4310,  0.2203,  0.0047, -0.4249]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys.transpose(-1,-2)"
      ],
      "metadata": {
        "id": "slBUTblxbgp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_batch"
      ],
      "metadata": {
        "id": "O6ddlTjpbzqz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb21a614-d1dc-4d81-9b3c-a7edf83f23ad"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.4300, 0.1500, 0.8900, 0.5500, 0.8700, 0.6600],\n",
              "         [0.5700, 0.8500, 0.6400, 0.2200, 0.5800, 0.3300],\n",
              "         [0.7700, 0.2500, 0.1000, 0.0500, 0.8000, 0.5500]],\n",
              "\n",
              "        [[0.4300, 0.1500, 0.8900, 0.5500, 0.8700, 0.6600],\n",
              "         [0.5700, 0.8500, 0.6400, 0.2200, 0.5800, 0.3300],\n",
              "         [0.7700, 0.2500, 0.1000, 0.0500, 0.8000, 0.5500]]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d12eODP6k221"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
