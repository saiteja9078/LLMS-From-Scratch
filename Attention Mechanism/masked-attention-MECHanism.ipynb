{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "id": "iPVMJihq4W_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNCSG8il3cu8",
        "outputId": "ab611905-bd04-49f3-d111-03091c37d55b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 3])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor([\n",
        "    [0.43, 0.15, 0.891],  # \"Your\"\n",
        "    [0.55, 0.87, 0.66],   # \"journey\"\n",
        "    [0.57, 0.85, 0.641],  # \"starts\"\n",
        "    [0.22, 0.58, 0.331], # \"with\"\n",
        "    [0.77, 0.25, 0.101],  # \"one\"\n",
        "    [0.05, 0.80, 0.551]   # \"step\"\n",
        "])\n",
        "\n",
        "print(inputs.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self,in_dim,out_dim):\n",
        "    super().__init__()\n",
        "    self.W_K = nn.Linear(in_dim,out_dim,bias=False)\n",
        "    self.W_V = nn.Linear(in_dim,out_dim,bias=False)\n",
        "    self.W_Q = nn.Linear(in_dim,out_dim,bias=False)\n",
        "  def forward(self,x):\n",
        "    keys =self.W_K(x)\n",
        "    values = self.W_V(x)\n",
        "    queries =self.W_Q(x)\n",
        "\n",
        "    attention_scores = queries @ keys.T\n",
        "    attention_weights = torch.softmax(attention_scores/keys.shape[-1]**0.5,dim=-1)\n",
        "\n",
        "    return attention_weights @ values"
      ],
      "metadata": {
        "id": "1Egk1wR62YbJ"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self_attention = SelfAttention(inputs.shape[-1],2)\n",
        "keys =self_attention.W_K(inputs)\n",
        "values = self_attention.W_V(inputs)\n",
        "queries =self_attention.W_Q(inputs)"
      ],
      "metadata": {
        "id": "COCrIJEUMOJb"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_scores = queries @ keys.T\n",
        "attention_weights = torch.softmax(attention_scores/keys.shape[-1]**0.5,dim=-1)\n",
        "attention_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYp-A7GnMrv9",
        "outputId": "638d6c59-f267-49b7-b6fd-a81f396ea0ff"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1745, 0.1637, 0.1639, 0.1650, 0.1702, 0.1626],\n",
              "        [0.1675, 0.1694, 0.1697, 0.1624, 0.1705, 0.1606],\n",
              "        [0.1671, 0.1696, 0.1698, 0.1624, 0.1704, 0.1607],\n",
              "        [0.1670, 0.1685, 0.1686, 0.1640, 0.1689, 0.1630],\n",
              "        [0.1607, 0.1714, 0.1714, 0.1646, 0.1665, 0.1653],\n",
              "        [0.1705, 0.1671, 0.1674, 0.1633, 0.1706, 0.1612]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNVFOFG-Ojel",
        "outputId": "eb3858ea-2b46-4b7b-e19c-23fa9e6a54b8"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1745, 0.1637, 0.1639, 0.1650, 0.1702, 0.1626],\n",
              "        [0.1675, 0.1694, 0.1697, 0.1624, 0.1705, 0.1606],\n",
              "        [0.1671, 0.1696, 0.1698, 0.1624, 0.1704, 0.1607],\n",
              "        [0.1670, 0.1685, 0.1686, 0.1640, 0.1689, 0.1630],\n",
              "        [0.1607, 0.1714, 0.1714, 0.1646, 0.1665, 0.1653],\n",
              "        [0.1705, 0.1671, 0.1674, 0.1633, 0.1706, 0.1612]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.triu(torch.full(attention_weights.shape, -torch.inf), diagonal=1)\n",
        "attention_scores = attention_weights+mask"
      ],
      "metadata": {
        "id": "qrFl6awtOljw"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_weights = torch.softmax(attention_scores/keys.shape[-1]**0.5,dim=-1)\n",
        "attention_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQFqd3FNSVj5",
        "outputId": "d409c757-4b6a-476e-8c31-ddf8af9c36e9"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.4996, 0.5004, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3329, 0.3335, 0.3336, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2503, 0.2503, 0.2495, 0.0000, 0.0000],\n",
              "        [0.1991, 0.2006, 0.2006, 0.1997, 0.1999, 0.0000],\n",
              "        [0.1671, 0.1667, 0.1667, 0.1663, 0.1671, 0.1660]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Masked attention\n",
        "attention_weights @ values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHVc2zNTT7_o",
        "outputId": "c6224a6d-bd1e-409d-ee8b-0a9b3b95c9fd"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.7395,  0.1178],\n",
              "        [-0.8368,  0.3166],\n",
              "        [-0.8662,  0.3838],\n",
              "        [-0.7741,  0.3621],\n",
              "        [-0.7177,  0.3745],\n",
              "        [-0.7021,  0.3596]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dropout = nn.Dropout(0.35)"
      ],
      "metadata": {
        "id": "d4dRsVT6T8za"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dropout(attention_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFVyiWbDWd-s",
        "outputId": "65442394-0d12-48ca-8b4c-1c0d56f788ff"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.5385, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.7687, 0.7698, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5122, 0.5131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.3850, 0.3850, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3063, 0.3087, 0.0000, 0.3072, 0.3076, 0.0000],\n",
              "        [0.0000, 0.2565, 0.2565, 0.2558, 0.2571, 0.2554]],\n",
              "       grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now gonna prcess inputs in batches\n",
        "input_batch = torch.stack((inputs,inputs),dim=0)"
      ],
      "metadata": {
        "id": "TsVcb7YkYBuz"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedSelfAttention(nn.Module):\n",
        "  def __init__(self,in_dim,out_dim,context_len,bias=False,dropout=0.25):\n",
        "    super().__init__()\n",
        "    self.d_out = out_dim\n",
        "    self.W_K = nn.Linear(in_dim,out_dim,bias= False)\n",
        "    self.W_Q = nn.Linear(in_dim,out_dim,bias= False)\n",
        "    self.W_V = nn.Linear(in_dim,out_dim,bias= False)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.register_buffer('mask',torch.triu(torch.ones(context_len,context_len),diagonal=1))\n",
        "  def forward(self,x):\n",
        "    b, num_tokens , d_in = x.shape\n",
        "    keys = self.W_K(x)\n",
        "    queries = self.W_Q(x)\n",
        "    values = self.W_V(x)\n",
        "    attention_scores = queries @ keys.transpose(1,2)# here if shape is (2,6,2) , it will swap 1st index dimension and 2nd index dimension.\n",
        "    attention_scores.masked_fill_(\n",
        "        self.mask.bool()[:num_tokens,:num_tokens],-torch.inf\n",
        "    )\n",
        "    attention_weights  = torch.softmax(attention_scores/keys.shape[-1]**0.5,dim=-1)\n",
        "    attention_weights =self.dropout(attention_weights)\n",
        "    return attention_weights"
      ],
      "metadata": {
        "id": "czJTNifPWfyS"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_len = input_batch.shape[1]\n",
        "attention = MaskedSelfAttention(3,2,context_len)\n",
        "attention.forward(input_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsVJ25SzZZHl",
        "outputId": "020def02-4563-41d7-dc70-b071e5f3ea17"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.6692, 0.6641, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4479, 0.4426, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.3262, 0.0000, 0.3365, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2801, 0.0000, 0.2579, 0.2733, 0.0000, 0.0000],\n",
              "         [0.0000, 0.2264, 0.2267, 0.2209, 0.2299, 0.2178]],\n",
              "\n",
              "        [[1.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.6692, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.4426, 0.4429, 0.0000, 0.0000, 0.0000],\n",
              "         [0.3262, 0.3362, 0.3365, 0.3344, 0.0000, 0.0000],\n",
              "         [0.2801, 0.2581, 0.2579, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2116, 0.2264, 0.0000, 0.0000, 0.2299, 0.2178]]],\n",
              "       grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZImG4IU-simb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
