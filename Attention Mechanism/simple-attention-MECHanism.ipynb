{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "id": "iPVMJihq4W_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Implementing Simplified attention mechanism\n",
        "####consider following input sequencce as our embedding vector"
      ],
      "metadata": {
        "id": "AwFWKZEh3ndL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNCSG8il3cu8",
        "outputId": "06e42649-3ee5-41da-d4c7-022db49f8e9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 3])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor([\n",
        "    [0.43, 0.15, 0.891],  # \"Your\"\n",
        "    [0.55, 0.87, 0.66],   # \"journey\"\n",
        "    [0.57, 0.85, 0.641],  # \"starts\"\n",
        "    [0.22, 0.58, 0.331], # \"with\"\n",
        "    [0.77, 0.25, 0.101],  # \"one\"\n",
        "    [0.05, 0.80, 0.551]   # \"step\"\n",
        "])\n",
        "\n",
        "print(inputs.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lets take journey and calculate attention weights with rest of them\n",
        "\n",
        "query = inputs[1]\n",
        "attention_weights = torch.empty(inputs.shape[0])\n",
        "for i,x_i in enumerate(inputs):\n",
        "  attention_weights[i] = torch.dot(x_i,query)\n",
        "attention_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QCPLOgc5KEs",
        "outputId": "d1732a8f-dfdb-4b60-a90f-1100118745ca"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9551, 1.4950, 1.4761, 0.8441, 0.7077, 1.0872])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lets calculate attention weights for all\n",
        "\n",
        "query = inputs[1]\n",
        "attention_weights = torch.empty(inputs.shape[0])\n",
        "for i,x_i in enumerate(inputs):\n",
        "  attention_weights[i] = torch.dot(x_i,query)\n",
        "attention_weights"
      ],
      "metadata": {
        "id": "h4_dbC--907M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77369184-9a34-4b3a-c891-0844825be968"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9551, 1.4950, 1.4761, 0.8441, 0.7077, 1.0872])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we got attention weights for all queries\n",
        "attention_weights = inputs @ inputs.T\n",
        "attention_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlQW9_QXIGct",
        "outputId": "465a0675-144e-46fd-b244-441d07fb38b5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0013, 0.9551, 0.9437, 0.4765, 0.4586, 0.6324],\n",
              "        [0.9551, 1.4950, 1.4761, 0.8441, 0.7077, 1.0872],\n",
              "        [0.9437, 1.4761, 1.4583, 0.8306, 0.7161, 1.0617],\n",
              "        [0.4765, 0.8441, 0.8306, 0.4944, 0.3478, 0.6574],\n",
              "        [0.4586, 0.7077, 0.7161, 0.3478, 0.6656, 0.2942],\n",
              "        [0.6324, 1.0872, 1.0617, 0.6574, 0.2942, 0.9461]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "# apply softmax along last dimension (rows)\n",
        "softmaxed = F.softmax(attention_weights, dim=-1)"
      ],
      "metadata": {
        "id": "uuMZaPJvItLv"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_vectors = softmaxed@inputs"
      ],
      "metadata": {
        "id": "T7xK1CUtIyOQ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_vectors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JOg3kJyJU9o",
        "outputId": "585c246d-f99a-4a9d-ff40-d891275732a6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4420, 0.5930, 0.5798],\n",
              "        [0.4418, 0.6514, 0.5691],\n",
              "        [0.4431, 0.6496, 0.5679],\n",
              "        [0.4304, 0.6298, 0.5519],\n",
              "        [0.4671, 0.5910, 0.5275],\n",
              "        [0.4177, 0.6503, 0.5654]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "words = [\"Your\", \"journey\", \"starts\", \"with\", \"one\", \"step\"]\n",
        "\n",
        "fig = plt.figure(figsize=(8, 6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "for i, vec in enumerate(context_vectors):\n",
        "    ax.scatter(vec[0], vec[1], vec[2], label=words[i])\n",
        "    ax.text(vec[0], vec[1], vec[2], words[i])  # label point\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "ax.set_title(\"Context Vectors (3D)\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "owi_ZpBeKHdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x5gdTuSzKeXN"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VdVeALN1KjPs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
